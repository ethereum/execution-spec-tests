# Consuming Tests (Fixtures) Generated by execution-spec-tests

The `consume` command is a comprehensive testing framework that enables execution clients to validate their implementation against test fixtures generated by execution-spec-tests. It provides multiple consumption methods, each optimized for different testing scenarios and development workflows.

## Test Fixture Formats

@ethereum/execution-spec-tests generates JSON test fixtures in different formats that can be consumed by execution clients either directly or via Hive:

| Format | Consumed by the client | Location in `.tar.gz` release |
| --- | --- | --- |
| [State Tests](../test_formats/state_test.md) | directly via a `statetest`-like command<br/> (e.g., [go-ethereum/cmd/evm/staterunner.go](https://github.com/ethereum/go-ethereum/blob/509a64ffb9405942396276ae111d06f9bded9221/cmd/evm/staterunner.go#L35)) | `./fixtures/state_tests/` |
| [Blockchain Tests](../test_formats/blockchain_test.md) | directly via a `blocktest`-like command<br/> (e.g., [go-ethereum/cmd/evm/blockrunner.go](https://github.com/ethereum/go-ethereum/blob/509a64ffb9405942396276ae111d06f9bded9221/cmd/evm/blockrunner.go#L39)) | `./fixtures/blockchain_tests/` |
| [Blockchain Engine Tests](../test_formats/blockchain_test_engine.md) | in the [Hive `pyspec` simulator](https://github.com/ethereum/execution-spec-tests/blob/main/src/pytest_plugins/consume/hive_simulators/engine/test_via_engine.py) via the Engine API and other RPC endpoints  | `./fixtures/blockchain_tests_engine/` |
| [Transaction Tests](../test_formats/transaction_test.md) | directly via a `t9`-like command<br/> (e.g., [go-ethereum's `evm t9`](https://github.com/ethereum/go-ethereum/tree/67a3b087951a3f3a8e341ae32b6ec18f3553e5cc/cmd/evm#transaction-tool)) | `./fixtures/transaction_tests/` |

## Available Consumption Methods

The consume command supports four distinct methods for running test fixtures:

| Method | Description | Use Case | Scope |
| --- | --- | --- | --- |
| [`direct`](./consume_command.md#direct-method) | Executes tests via the `blocktest` interface | Quick module testing with fast feedback | Module test |
| [`rlp`](./rlp.md) | Consumes RLP-encoded blocks directly | Testing block processing and import logic | System test |
| [`engine`](./engine.md) | Uses the Engine API for test execution | Testing post-merge functionality | System test |
| [Hive simulators](./hive.md) | Runs tests via containerized Hive simulators | Comprehensive client testing | Integration test |

Here's a top-level comparison of the different methods of consuming tests:

| Consumed via | Scope | Pros | Cons |
| --- | --- | --- | --- |
| `statetest` or <code>blocktest</code>-like command | Module test | - Fast feedback loop<br/>- Less complex | - Smaller coverage scope<br/>- Requires a dedicated interface to the client EVM to consume the JSON fixtures and execute tests |
| EEST `consume rlp/engine` | System test | - Tests full client stack<br/>- Real-world code paths<br/>- Comprehensive validation | - Slower than module tests<br/>- More complex setup |
| Hive simulators | Integration test | - Widest coverage scope<br/>- Containerized isolation<br/>- Multi-client testing | - Slowest feedback loop<br/>- Most complex to debug<br/>- Requires Docker setup |

## RLP vs Engine Simulator Comparison

### Why Both Simulators Are Needed

**Ideally both simulators should be executed for full coverage.** These methods use different fixture formats but are generated from the same Python test source, so they test the same logic but cover different code paths within execution clients.

### Code Path Differences

| Aspect | RLP Simulator (`consume-rlp`) | Engine Simulator (`consume-engine`) |
|--------|-------------------------------|-------------------------------------|
| **Client code path** | Historical sync / block import pipeline | Engine API / consensus integration |
| **Real-world analogy** | Blocks received during sync | Blocks received from consensus client |
| **Fork support** | All forks (including pre-merge) | Post-merge forks only (Paris+) |
| **Interface** | Direct block import via RLP files | Engine API calls (newPayload, forkchoiceUpdated) |
| **Networking simulation** | None (direct import) | Simulates consensus client interaction |
| **Exception testing** | Basic exception handling | **Advanced exception verification with client-specific mappers** |

### When to Use Each Method

**RLP Simulator is ideal for:**

- Testing historical sync code paths
- Validating block import and validation logic
- Testing all forks including pre-merge
- Debugging block processing issues
- Performance testing of block import

**Engine Simulator is essential for:**

- Testing production post-merge behavior
- Validating Engine API implementations
- Testing consensus-execution layer interaction
- Ensuring correct payload handling
- **Performing exception tests with correct client exception verification**
- Production readiness validation

### Practical Implications for Client Teams

**For comprehensive testing, client teams should:**

1. **Run both simulators** to ensure full code path coverage
2. **Use RLP for historical sync testing** - it exercises the same code path used during historical synchronization
3. **Use Engine for production validation** - it tests the native interface used in production mainnet
4. **Leverage Engine for exception testing** - it includes client-specific exception mappers for accurate error validation
5. **Implement dual code path testing** - modify your consumer to test the same payload through both historical sync and Engine API paths

**Diagnostic value:** If Engine tests fail but RLP tests pass, this indicates the issue is in Engine API implementation rather than core EVM consensus logic.

## Choosing the Right Method

- **For rapid development cycles**: Use `direct` method for immediate EVM-level feedback
- **For historical sync testing**: Use `rlp` method to test block import code paths
- **For Engine API testing**: Use `engine` method to test post-merge consensus integration
- **For exception validation**: Use `engine` method with client-specific exception mappers
- **For comprehensive validation**: Use both `rlp` and `engine` via Hive simulators
- **For CI/CD integration**: Use Hive simulators with automated client configurations
- **For multi-client comparison**: Use Hive simulators to test multiple clients simultaneously

## Quick Start

```bash
# Run tests using the direct method (fastest)
uv run consume direct --input ./fixtures --traces

# Run tests using RLP method (historical sync paths)
uv run consume rlp --input ./fixtures --timing-data

# Run tests using Engine API (post-merge production paths + exception testing)
uv run consume engine --input ./fixtures --fork cancun

# Run comprehensive tests via Hive simulators
./hive --sim ethereum/eest/consume-rlp --client go-ethereum
./hive --sim ethereum/eest/consume-engine --client go-ethereum
```

!!! note "Running `blocktest`, `statetest`, directly within the execution-spec-tests framework"

    It's possible to execute `evm blocktest` directly within the execution-spec-tests framework. This is intended to verify fixture generation, see [Debugging `t8n` Tools](../../filling_tests/debugging_t8n_tools.md).

!!! note "Generating test fixtures using a `t8n` tool via `fill` is not considered to be the actual test"

    The `fill` command uses `t8n` tools to generate fixtures. Whilst this will provide basic sanity checking of EVM behavior and a sub-set of post conditions are typically checked within test cases, it is not considered the actual test. The actual test is the execution of the fixture against the EVM which will check the entire post allocation and typically use different code paths than `t8n` commands.

## Release Formats

The @ethereum/execution-spec-tests repository provides [releases](https://github.com/ethereum/execution-spec-tests/releases) of fixtures in various formats:

| Release Artifact               | Consumer | Fork/feature scope |
| ------------------------------ | -------- | ------------------ |
| `fixtures_stable.tar.gz`       | Clients  | All tests until the last stable fork ("must pass") |
| `fixtures_develop.tar.gz`      | Clients  | All tests until the last development fork |

## Fixture Input Sources

EEST provides a flexible `--input` system for specifying test fixture sources. See [Specifying Consume Input Sources](./input_sources.md) for detailed information about:

- **Local directories**: Using fixtures from your file system
- **Release specifications**: `stable@latest`, `develop@v4.1.0`, etc.
- **Direct URLs**: GitHub releases and custom archives  
- **Automatic caching**: Downloaded fixtures are cached in `~/.cache/`
- **Cache management**: Using `consume cache` to pre-download fixtures

## Continuous Integration and Automated Testing

The Ethereum Foundation maintains continuous testing infrastructure:

- **Production results**: [hive.ethpandaops.io](https://hive.ethpandaops.io)
- **GitHub Actions**: [ethpandaops/hive-github-action](https://github.com/ethpandaops/hive-github-action)

For detailed information about each method, see the individual documentation pages linked above.
